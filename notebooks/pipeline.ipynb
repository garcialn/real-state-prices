{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e404a41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Real State Properties Price Prediction  \n",
    "---\n",
    "\n",
    "### Project for prediction of Real State properties price  based on the features:\n",
    "\n",
    "\n",
    "- City (Predictor)\n",
    "- Type of property (Predictor)\n",
    "- Address (Predictor)\n",
    "- Neighborhood (Predictor)\n",
    "- Footage (Predictor)\n",
    "- Doorms (Predictor)\n",
    "- Garages (Predictor)\n",
    "- Price (Dependent Variable)\n",
    "---\n",
    "\n",
    "### We're gonna start our project using notebooks so that we can do it straight forward, after we're done here, we're gonna move into the source code of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22a3f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## importing the libs for the project\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import duckdb as duck\n",
    "import pyarrow as pa\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from matplotlib import pyplot as plt\n",
    "import dataclasses\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression, GammaRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import mlflow\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7da078",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON > Parquet\n",
    "Before starting the process of cleaning and transforming the data for  our analysis, we're gonna make sure to convert the files into `.parquet` format so that we're always dealing with optimazed performance datasets, no matter the situation.  \n",
    "\n",
    "For that, we're gonna start by loading our data into [Pola.rs](pola.rs 'Most Efficient DataFrame Lib for Python') dataframes and try to get some information from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c20129",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## read the .json files and convering it to .parquet\n",
    "\n",
    "sp_dt = pl.read_json('/home/garcia-ln/Documentos/real-state-prices/data/raw/sp_properties.json')\n",
    "rj_dt = pl.read_json('/home/garcia-ln/Documentos/real-state-prices/data/raw/rj_properties.json')\n",
    "pa_dt = pl.read_json('/home/garcia-ln/Documentos/real-state-prices/data/raw/pa_properties.json')\n",
    "bh_dt = pl.read_json('/home/garcia-ln/Documentos/real-state-prices/data/raw/bh_properties.json')\n",
    "\n",
    "sp_dt.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/processed/sp_properties.parquet')\n",
    "rj_dt.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/processed/rj_properties.parquet')\n",
    "pa_dt.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/processed/pa_properties.parquet')\n",
    "bh_dt.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/processed/bh_properties.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e56de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## using a duckdb and SQL queries\n",
    "\n",
    "con = duck.connect(database='/home/garcia-ln/Documentos/real-state-prices/data/processed/real_state.duckdb')\n",
    "\n",
    "sql_sp = '''\n",
    "    CREATE TABLE sp_tbl as \n",
    "        SELECT * FROM '~/Documentos/real-state-prices/data/processed/sp_properties.parquet';\n",
    "    ALTER TABLE sp_tbl\n",
    "        ADD COLUMN city VARCHAR DEFAULT 'Sao_Paulo'\n",
    "'''\n",
    "\n",
    "sql_rj = '''\n",
    "    CREATE TABLE rj_tbl as \n",
    "         SELECT * FROM '~/Documentos/real-state-prices/data/processed/rj_properties.parquet';\n",
    "    ALTER TABLE rj_tbl\n",
    "        ADD COLUMN city VARCHAR DEFAULT 'Rio_de_Janeiro'\n",
    "'''\n",
    "\n",
    "sql_pa = '''\n",
    "    CREATE TABLE pa_tbl as \n",
    "        SELECT * FROM '~/Documentos/real-state-prices/data/processed/pa_properties.parquet';\n",
    "    ALTER TABLE pa_tbl\n",
    "        ADD COLUMN city VARCHAR DEFAULT 'Porto_Alegre'\n",
    "'''\n",
    "\n",
    "sql_bh = '''\n",
    "    CREATE TABLE bh_tbl as \n",
    "        SELECT * FROM '~/Documentos/real-state-prices/data/processed/bh_properties.parquet';\n",
    "    ALTER TABLE bh_tbl\n",
    "        ADD COLUMN city VARCHAR DEFAULT 'Belo_Horizonte'\n",
    "'''\n",
    "\n",
    "con.execute(sql_sp).fetchall()\n",
    "sp_df = con.table('sp_tbl').df()\n",
    "display(sp_df)\n",
    "\n",
    "\n",
    "con.execute(sql_rj).fetchall()\n",
    "rj_df = con.table('rj_tbl').df()\n",
    "display(rj_df)\n",
    "\n",
    "\n",
    "con.execute(sql_pa).fetchall()\n",
    "pa_df = con.table('pa_tbl').df()\n",
    "display(pa_df)\n",
    "\n",
    "\n",
    "con.execute(sql_bh).fetchall()\n",
    "bh_df = con.table('bh_tbl').df()\n",
    "display(bh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b59411",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## using SQL queries to create one unified table\n",
    "\n",
    "sql = '''\n",
    "    CREATE TABLE properties as\n",
    "        SELECT * FROM sp_tbl \n",
    "    UNION ALL \n",
    "        SELECT * FROM rj_tbl \n",
    "    UNION ALL \n",
    "        SELECT * FROM pa_tbl \n",
    "    UNION ALL \n",
    "        SELECT * FROM bh_tbl;\n",
    "    ORDERBY random()\n",
    "'''\n",
    "\n",
    "con.execute(sql).fetchall()\n",
    "\n",
    "properties = pl.from_pandas(con.table('properties').df())\n",
    "properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d0512",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dtypes\n",
    "\n",
    "Now that we altered the file from `.json` to `.parque` and added the feature to our dataset we're gonna **add all the tables together and define the dtypes of our data**.  \n",
    "\n",
    "\n",
    "After that we're gonna make sure to **change all dtypes of our dataset**, to keep a tidy dataset for our cleaning, analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49645736",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## the code for change dtypes on Polars\n",
    "\n",
    "properties = properties.with_columns(\n",
    "   [\n",
    "       (pl.col('type').cast(pl.Categorical)),\n",
    "       (pl.col('city').cast(pl.Categorical)),\n",
    "       (pl.col('neighborhood').cast(pl.Categorical)),\n",
    "       (pl.col('footage').cast(pl.Int16)),\n",
    "       (pl.col('doorms').cast(pl.Int8)),\n",
    "       (pl.col('garages').cast(pl.Int8)),\n",
    "       (pl.col('price').cast(pl.Int32))\n",
    "   ]\n",
    ")\n",
    "\n",
    "properties.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/raw/properties.parquet')\n",
    "properties = pl.read_parquet('/home/garcia-ln/Documentos/real-state-prices/data/raw/properties.parquet')\n",
    "properties = properties.drop('address').sample(frac=1, shuffle=True, seed=42).select(\n",
    "    [\n",
    "        'city', 'neighborhood', 'type', 'footage', 'doorms', 'garages', 'price'\n",
    "    ]\n",
    ")\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073b744",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## SQL query for changing the dtypes of the properties table\n",
    "\n",
    "con.execute('''\n",
    "    ALTER TABLE properties\n",
    "        DROP COLUMN adress;\n",
    "    ALTER TABLE properties\n",
    "        ALTER type SET DATA TYPE VARCHAR;\n",
    "    ALTER TABLE properties\n",
    "        ALTER city SET DATA TYPE VARCHAR;\n",
    "    ALTER TABLE properties\n",
    "        ALTER neighborhood SET DATA TYPE VARCHAR;\n",
    "    ALTER TABLE properties    \n",
    "        ALTER footage SET DATA TYPE SMALLINT;\n",
    "    ALTER TABLE properties    \n",
    "        ALTER doorms SET DATA TYPE INT2;\n",
    "    ALTER TABLE properties\n",
    "        ALTER garages SET DATA TYPE INT2;\n",
    "    ALTER TABLE properties    \n",
    "        ALTER price SET DATA TYPE INT4\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c192b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## EDA\n",
    "Now it's time for one of the most important part of a data job: the analysis. Here we're gonna focus on undertanding our data, **its features, dimensions, characteristics, distribution and interactions**.  \n",
    "\n",
    "We're gonna start by understanding the basic information on the qualy and quant variables, followed by some visualizations to help on the insights for our analysis.  \n",
    "\n",
    "What we want to check in our analysis (aside from business details that depends on the questions to be answered):  \n",
    "> - Type of dataset\n",
    ">> - Cross Section\n",
    ">> - Time Series\n",
    ">> - Painel Data\n",
    "> - Frequency, Distributions and Density of observations (undertanding the proportions of inputs and outputs for both numerical and categorical features)\n",
    "> - Missing values\n",
    "> - Outliers\n",
    "> - Dirty data\n",
    "> - Feature Engineering\n",
    ">> - Handling missing values, outliers, and cleaning the dataset\n",
    ">> - Scalling the data (Standardization, Normalization) \n",
    ">> - Decoding categorical features\n",
    "> - Specificity\n",
    ">> - Normality\n",
    ">> - Linearity\n",
    ">> - Means Interaction\n",
    ">> - Variance Interaction\n",
    ">> - Autocorrelation\n",
    ">> - Multicolinearity\n",
    ">> - Heteroskedasticity\n",
    "> - Feature Selection \n",
    ">> - Correlation\n",
    ">> - K Neighbor\n",
    ">> - ChiSquare\n",
    ">> - Genetic Algo\n",
    ">> - Feature Importance (Extra Tree Classifier)\n",
    "\n",
    "Lets make some changes on our dataset to make sure we'll be able to work on it. In this case, i'm gonna use `seaborn` for our dataviz (wich requires the DF on `pandas` format, and not `polars`) insted of using `plotly express` (wich we can use the `polars` DF and generates interactive plots). The reason for that, is for prettier dataviz made simple and easy, seaborn is the way to go and given that we don't have such a big df, there's no problem transforming the pl.df to pd.df just for plotting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f422c6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## checking the null values count and schema of the dataset\n",
    "\n",
    "display(\n",
    "    properties.null_count(),\n",
    "    properties.schema\n",
    ")\n",
    "\n",
    "null = properties.filter(pl.col('type')==None).to_pandas()\n",
    "df = properties.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd8179",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## checking the unique values of every relevant feature\n",
    "\n",
    "display(\n",
    "    properties['type'].value_counts(),\n",
    "    properties['doorms'].value_counts().sort('doorms'),\n",
    "    properties['garages'].value_counts().sort('garages'),\n",
    "    properties['city'].value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6002f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From this, we can observe that we have a very small volume of missing values. But before we do anything with it, lets check if we have those null values concentrated on a group, or if it's well distributed through all citys and prices. After that, we decide whether to **drop those null values, or make some statistical interpolation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02e81b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "source = 'Source: loft.com.br; Elaborated by author'\n",
    "\n",
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.catplot(\n",
    "    kind='count',\n",
    "    data=null, x='city', \n",
    "    height=4, aspect=1.3,  \n",
    "    orient='v', \n",
    "    alpha=.75, linewidth=1\n",
    ").set_axis_labels('City', 'Count').set_xticklabels(['SP', 'RJ', 'PA', 'BH'])\n",
    "plt.title('Null Count/City', fontname='Arial', size=15, fontweight=\"bold\")\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='center',\n",
    "    fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6b735",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    kind='box',\n",
    "    data=null, x='price', y='city',   \n",
    "    orient='h',\n",
    "    height=5, aspect=1.5\n",
    ").set_axis_labels('Price', 'City').set_xticklabels(\n",
    "    labels=['', '$0', '$500K', '$1M', '$1.5M', '$2M',\n",
    "            '$2.5M', '$3M', '$3.5M', '$4M'], \n",
    "    rotation=0\n",
    ")\n",
    "\n",
    "plt.title('Null Prices/City', fontname='Arial', size=15, fontweight='bold')\n",
    "plt.annotate(\n",
    "    source, \n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='center',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910294b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hist=so.Plot(\n",
    "        null, \n",
    "        x=\"price\", \n",
    "        y=None, \n",
    "        color=\"city\").add(\n",
    "                so.Bars(), \n",
    "                so.Hist(bins=15, stat='count'), \n",
    "                so.Dodge(),\n",
    "                )\n",
    "\n",
    "hist.label(title='Null Price/City', x='Price', y='Percentage').layout(size=(10, 5)).scale(\n",
    "        x = so.Continuous().tick(every=500_000).label(like='${x:,.0f}')\n",
    ").limit(x=(0, 3_500_000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68020a06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This plots can make easy for us to understand that São Paulo has the majority of null type records, that's important for us when we proceed to use some interpolation to fill those empty data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9bd9c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.displot(data=df, x='price', hue='type', col='city', col_wrap=2, \n",
    "    kind='hist', log_scale=True, element='step', \n",
    "    palette='ch:s=1, r=2, l=.3, d=.5', height=5, aspect=1\n",
    ").set_axis_labels('Prices(log)')\n",
    "\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcca2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.displot(data=df, x='footage', hue='type', col='city', col_wrap=2, \n",
    "    kind='hist', log_scale=True, element='step', \n",
    "    palette='ch:s=1, r=2, l=.3, d=.5', height=5, aspect=1\n",
    ").set_axis_labels('Footage(log)')\n",
    "\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01701bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.relplot(\n",
    "    data=df, x='price', y='footage', kind='scatter',\n",
    "    hue='type', col='city', col_wrap=2,\n",
    "    height=5, aspect=1\n",
    ").set_axis_labels('Prices (R$)', 'Footage (m²)').set_xticklabels(\n",
    "    labels=['', '$0', '$5M', '$10M', '$15M', '$20M',\n",
    "            '$25M', '$30M', '$35M', '$40M', '$45M'], \n",
    "    rotation=45\n",
    ")\n",
    "\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff5156",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Now we did some analysis on our dataset, we can take some conclusions, having more information on what to do next:  \n",
    "\n",
    "We are focus on predicting properties prices based on just a few features (but over all important on our model). Having said that, we notice that we have some **Null values, outliers (Footage, Doorms, Garages and prices) and some categorical observations that we can dropout for being in such small number of observations (types of properties)**. We can do that using the Z-score, or quartile interval \n",
    "- Properties types = Apartment, Houses, Studios and Rooftop\n",
    "- Footage < 1.000m²\n",
    "- Doorms < 6\n",
    "- Garages < 8 \n",
    "- Prices < R$ 10.000.000\n",
    "\n",
    "we're gonna start the feature engineering before the statistical tests stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we're defining a function to get the quantiles to get the outliers and see how to treat it\n",
    "\n",
    "def get_quantiles(df: pl.DataFrame, upper: float, lower: float) -> list[pl.DataFrame]:\n",
    "    '''\n",
    "        Get the quantiles values given the Dataframe, upper and lower limit\n",
    "    '''\n",
    "    upper_quantile = df.quantile(quantile=upper)\n",
    "    lower_quantile = df.quantile(quantile=lower)\n",
    "    return [upper_quantile, lower_quantile]\n",
    "\n",
    "def filter_outliers(\n",
    "    df: pl.DataFrame, \n",
    "    column: str, \n",
    "    upper_quantile: float, \n",
    "    lower_quantile: float\n",
    ") -> pl.DataFrame:\n",
    "    '''\n",
    "        Filter the values between the quantiles\n",
    "    '''\n",
    "    amp = upper_quantile - lower_quantile\n",
    "    iqr_sup = upper_quantile + (1.5 * amp)\n",
    "    iqr_inf = lower_quantile - (1.5 * amp) \n",
    "    df = df.filter(\n",
    "        (pl.col(column) < iqr_sup) &\n",
    "        (pl.col(column) > iqr_inf)\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4899ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(properties_eng, .75, .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = filter_outliers(properties, 'doorms', 3, 2)\n",
    "test = filter_outliers(test, 'garages', 2, 1)\n",
    "test = filter_outliers(properties, 'footage', 125, 57)\n",
    "test = filter_outliers(properties, 'doorms', 3, 2)\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c8658",
   "metadata": {},
   "source": [
    "We've used the inter quantile method to get the outliers, and since we have a pretty small number of outliers and we have a preference to develop models focus on a certain interval of values (we're gonna use the same strategy that Loft, which is the place from where we got our data). So we're gonna try to get our model to predict only for values **between 150k and 1M**.\n",
    "\n",
    "Wen can do that since we don't have too many outliers and the idea of out model isn't anomalies detection, but a regression using cross section data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d86e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping irrelevant feature (address) which will be replaced by neighborhood\n",
    "\n",
    "properties_clean = pl.read_parquet('/home/garcia-ln/Documentos/real-state-prices/data/raw/properties.parquet')\n",
    "properties_clean = properties_clean.drop('address').sample(frac=1, shuffle=True, seed=42).select(\n",
    "    [\n",
    "        'city', 'neighborhood', 'type', 'footage', 'doorms', 'garages', 'price'\n",
    "    ]\n",
    ")\n",
    "\n",
    "properties_clean.columns = ['city', 'address', 'type', 'footage', 'doorms', 'garages', 'price']\n",
    "\n",
    "## now we're gonna fill the null values using forward method, since we've checked the null values are so small\n",
    "## and don't have particular characteristics\n",
    "\n",
    "properties_clean = properties_clean.fill_null(strategy='forward').filter(\n",
    "    (pl.col('type') == 'Apartamento') | \n",
    "    (pl.col('type') == 'Casa') | \n",
    "    (pl.col('type') == 'Cobertura') |\n",
    "    (pl.col('type') == 'Duplex') |\n",
    "    (pl.col('type') == 'Studio')\n",
    ")\n",
    "\n",
    "properties_clean = properties_clean.with_columns(\n",
    "   [\n",
    "       (pl.col('type').cast(pl.Categorical)),\n",
    "       (pl.col('city').cast(pl.Categorical)),\n",
    "       (pl.col('address').cast(pl.Categorical)),\n",
    "       (pl.col('footage').cast(pl.Int16)),\n",
    "       (pl.col('doorms').cast(pl.Int8)),\n",
    "       (pl.col('garages').cast(pl.Int8)),\n",
    "       (pl.col('price').cast(pl.Int32))\n",
    "   ]\n",
    ")\n",
    "\n",
    "## here we're gonna set some threshold values to filter based on the \n",
    "## independent variables price, footage, doorms and garages\n",
    "\n",
    "properties_clean = properties_clean.filter(\n",
    "    (pl.col('price') <= 1_000_000) & \n",
    "    (pl.col('price') >= 100_000) &\n",
    "    (pl.col('footage') >= 0) & \n",
    "    (pl.col('footage') <= 250) &\n",
    "    (pl.col('doorms') >= 1) &\n",
    "    (pl.col('doorms') <= 5) &\n",
    "    (pl.col('garages') <= 5)\n",
    ")\n",
    "\n",
    "display(\n",
    "    properties_clean,\n",
    "    properties_clean['type'].value_counts(),\n",
    "    properties_clean.describe()\n",
    ")\n",
    "\n",
    "properties_clean.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/processed/properties_clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d253ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_clean = pl.read_parquet('/home/garcia-ln/Documentos/real-state-prices/data/processed/properties_clean.parquet')\n",
    "df_clean = properties_clean.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13a635",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.relplot(\n",
    "    data=properties_clean, x='price', y='footage', kind='scatter',\n",
    "    col='city', col_wrap=2, hue='type',\n",
    "    height=4, aspect=1\n",
    ").set_axis_labels('Prices (R$)', 'Footage (m²)').set_xticklabels(\n",
    "    labels=['$0', '$200K', '$400K', '$600K', '$800K', '$1M', ''], \n",
    "    rotation=0\n",
    ")\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.3),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce015a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.displot(data=properties_clean, x='price', col='city', col_wrap=2, kde=True,\n",
    "    kind='hist', log_scale=False, element='step', height=5, aspect=1\n",
    ").set_axis_labels('Prices(R$1,000)').set_xticklabels(\n",
    "    labels=['0', '200', '400', '600', '800', '1,000', ''], \n",
    "    rotation=0\n",
    ")\n",
    "\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1,\n",
    ")\n",
    "\n",
    "sns.displot(data=properties_clean, x='footage', col='city', col_wrap=2, kde=True,\n",
    "    kind='hist', log_scale=False, element='step', height=5, aspect=1\n",
    ").set_axis_labels('Footage (m²)')\n",
    "\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff4c05-f410-488d-8c78-0188430a09ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After we've checked the overall characteristics of our dataset, we can proceed with some changes for our model development stage.   \n",
    "\n",
    "As we've seen, the first thing after having diagnosed of our data is to perform Feature Selection (wich can be done both before we load the data into our development environment (as a SQL Query on some DW, usually) and after we load into our dataset (as we did here).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef99ef4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1\n",
    ")\n",
    "\n",
    "sns.catplot(\n",
    "    kind='box',\n",
    "    data=df_clean, y='price', x='city',\n",
    "    orient='v', height=5, aspect=1.3\n",
    ").set_axis_labels('Cities', 'Prices').set_yticklabels(\n",
    "    labels=['$0', '$200K', '$400K', '$600K', '$800K', '$1M', ''], \n",
    "    rotation=0\n",
    ")\n",
    "\n",
    "plt.title('Boxplot Prices/City', fontname='Arial', size=15, fontweight=\"bold\")\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e01ecfc",
   "metadata": {},
   "source": [
    "Now we're gonna encode and scale our features just for good practice, since our intention is using **Xgboost and Lightgbm (wich are ensembled models by gradient boosting that don't require these kinds of feature engineering)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33073e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here we're gonna encode the entire values of categorical data, so that our model performance can be maximize\n",
    "\n",
    "## lets start by changing it to pandas Dataframe, for no problems with sklearn and separating x from y\n",
    "\n",
    "y = properties_clean['price'].to_pandas()\n",
    "X1 = properties_clean.drop(columns=['price']).to_pandas()\n",
    "\n",
    "## we're gonna use OrdinalEncoder since the address feature would create too many dummies variables\n",
    "\n",
    "ord_enc = OrdinalEncoder()\n",
    "X2 = ord_enc.fit_transform(X1[['city', 'address', 'type']])\n",
    "X2 = pd.DataFrame(X2, columns=['city_encoded', 'address_encoded', 'type_encoded'])\n",
    "X3 = X1.join(X2).drop(['city', 'type'], axis=1)\n",
    "X3 = pl.DataFrame(X3).select(\n",
    "    [\n",
    "        'city_encoded',  \n",
    "        'address_encoded',\n",
    "        'type_encoded', \n",
    "        'footage', \n",
    "        'doorms',\n",
    "        'garages'\n",
    "    ]\n",
    ")\n",
    "\n",
    "X3.columns = [\n",
    "    'city', \n",
    "    'address',    \n",
    "    'type', \n",
    "    'footage', \n",
    "    'doorms',\n",
    "    'garages'\n",
    "    ]\n",
    "X3 = X3.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e532b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_final = pl.from_pandas(X3.join(y)).with_columns(\n",
    "   [\n",
    "       (pl.col('city').cast(pl.Int8)),\n",
    "       (pl.col('address').cast(pl.Int16)),\n",
    "       (pl.col('type').cast(pl.Int8)),\n",
    "       (pl.col('footage').cast(pl.Float32)),\n",
    "       (pl.col('doorms').cast(pl.Int8)),\n",
    "       (pl.col('garages').cast(pl.Int8)),\n",
    "       (pl.col('price').cast(pl.Int32))\n",
    "   ]\n",
    ")\n",
    "properties_final.write_parquet('/home/garcia-ln/Documentos/real-state-prices/data/final/properties_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a714de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_final = pl.read_parquet('/home/garcia-ln/Documentos/real-state-prices/data/final/properties_final.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5bf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = properties_final['price'].to_pandas()\n",
    "X_val = properties_final.drop(columns=['price']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spliting the train-test datasets (if we were to perform hyperparameter tunning, we would also split\n",
    "## the train dataset into 2 datasets: train, validation and test datasets)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_val, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "## here scal the x dataset\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit(X).transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=['city', 'address', 'type', 'footage', 'doorms', 'garages'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ae5aa",
   "metadata": {},
   "source": [
    "### Statistical Tests\n",
    "\n",
    "Since **we're gonna use Xgboost and Lightgbm algorithms for regression, which are not parametrized (in other words, it doesn't have assumptions aplied to the data). So it's not necessary for us to perform the majority of the statistical tests** that would be necessary for OLS, ARIMA, SARIMA, PCA, SVM, etc...  \n",
    "\n",
    "Those tests are necessary to understand better (accept or deny the assumptions) the data that we have, how should we treat it and what estimator to use for better and more robust models.  \n",
    "\n",
    "Since it's not the case, the only test we're gonna perform is to **check colinearity throught correlation matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1.1,\n",
    ")\n",
    "\n",
    "sns.heatmap( \n",
    "    data=X3.corr(\n",
    "        method='pearson', \n",
    "        numeric_only=True\n",
    "    ),\n",
    "    linewidths=.5, \n",
    "    linecolor='w', \n",
    "    annot=True,\n",
    "    fmt='.2f'\n",
    ")\n",
    "    \n",
    "plt.title('Heatmap: Pearson Corr. Test', fontname='Arial', size=15, fontweight='bold')\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a475b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context='notebook', \n",
    "    style='darkgrid', \n",
    "    font_scale=1.1,\n",
    ")\n",
    "\n",
    "sns.heatmap( \n",
    "    data=X3.corr(\n",
    "        method='spearman', \n",
    "        numeric_only=True\n",
    "    ),\n",
    "    linewidths=.5, \n",
    "    linecolor='w', \n",
    "    annot=True,\n",
    "    fmt='.2f'\n",
    ")\n",
    "    \n",
    "plt.title('Heatmap: Spearman Corr. Test', fontname='Arial', size=15, fontweight='bold')\n",
    "plt.annotate(\n",
    "    source,\n",
    "    xy=(1, -.2),\n",
    "    xycoords='axes fraction',\n",
    "    ha='left',\n",
    "    fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37425952",
   "metadata": {},
   "source": [
    "The higher correlation is .75 between doorms and footage, which we can still consider the independent matrix linear independent (no multicolinearity detect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b380424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the validation metrics\n",
    "\n",
    "def eval_metrics(actual: pd.DataFrame, predict: pd.DataFrame) -> list[float]:\n",
    "    '''\n",
    "        Defining the 3 evaluation metrics, given actual and predicted values: \n",
    "            - Root Mean Squared Error\n",
    "            - Mean Absolute Error\n",
    "            - R²\n",
    "    '''\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predict))\n",
    "    mae = mean_absolute_error(actual, predict)\n",
    "    r2 = r2_score(actual, predict)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbf4b3",
   "metadata": {},
   "source": [
    "###### Before we use mlflow to log the experiments, we're gonna use cross validation so we don't overfit the model and get unwanted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6da5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_reg = LGBMRegressor(learning_rate=.3, max_depth=13, boosting_type='gbdt')\n",
    "xgb_reg = XGBRegressor(learning_rate=.3, max_depth=6, booster='gbtree')\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "\n",
    "cv_lgbm = cross_val_score(estimator=lgbm_reg, X=X_train, y=y_train, scoring='neg_root_mean_squared_error', cv=cv).mean()\n",
    "cv_xgb = cross_val_score(estimator=xgb_reg, X=X_train, y=y_train, scoring='neg_root_mean_squared_error', cv=cv).mean()\n",
    "\n",
    "cv_lgbm, cv_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# search_space = {\n",
    "#     'learning_rate': Real(.1, 1),\n",
    "#     'max_depth': Integer(-1, 15),\n",
    "#     'boosting_type': Categorical(['gbdt', 'dart', 'goss', 'rf'])\n",
    "# }\n",
    "\n",
    "# lgbm_opt = BayesSearchCV(\n",
    "#     LGBMRegressor(),\n",
    "#         search_spaces=search_space,\n",
    "#     scoring='neg_root_mean_squared_error',\n",
    "#     n_iter=32,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "#     cv=10\n",
    "#  )\n",
    "\n",
    "# lgbm_opt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1883c6",
   "metadata": {},
   "source": [
    "### mlflow\n",
    "\n",
    "Now we're gonna create logs from the models, parameters, metrics and artifacts using **mlflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c61037",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting the experiment name\n",
    "\n",
    "experiment_name = 'RealState-Price-Prediction'\n",
    "\n",
    "## creating or getting the experiment\n",
    "\n",
    "try:\n",
    "    exp_id = mlflow.create_experiment(name=experiment_name)\n",
    "except Exception as e:\n",
    "    exp_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "    \n",
    "## params list\n",
    "    \n",
    "learning_rate_xgb, max_depth_xgb,  = .3, 6\n",
    "learning_rate_lgbm, max_depth_lgbm,  = .3, 13\n",
    "booster_xgb, booster_lgbm = 'gbtree', 'gbdt'\n",
    "eval_metric = 'rmse'\n",
    "seed = 42\n",
    "n_observations = X.shape[0]\n",
    "n_features = X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ff9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the context\n",
    "\n",
    "with mlflow.start_run(experiment_id=exp_id, run_name='notebook_run'):\n",
    "    \n",
    "    ## setting tags\n",
    "    \n",
    "    mlflow.set_tags(\n",
    "        {\n",
    "            'Problem': 'Regression',\n",
    "            'Models': 'Lightgbm'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ## this plot isn't for statistical analysis, but for a reference when looking at the UI\n",
    "    \n",
    "    sns.displot(data=properties_clean, x='price', col='city', col_wrap=2, kde=True,\n",
    "        kind='hist', log_scale=False, element='step', height=5, aspect=1\n",
    "    ).set_axis_labels('Price (R$ 1,000)').set_xticklabels(\n",
    "        labels=['0', '200', '400', '600', '800', '1,000', ''], \n",
    "        rotation=0\n",
    "    )\n",
    "    plt.savefig('/home/garcia-ln/Documentos/real-state-prices/images/price_hist.png')\n",
    "    \n",
    "    ## artifacts logging (images)\n",
    "    \n",
    "    mlflow.log_artifact('/home/garcia-ln/Documentos/real-state-prices/images/')\n",
    "    \n",
    "\n",
    "    ## lightgbm model\n",
    "    \n",
    "    lgbm_reg = LGBMRegressor(\n",
    "        max_depth=max_depth_lgbm, \n",
    "        learning_rate=learning_rate_lgbm, \n",
    "        boosting_type=booster_lgbm\n",
    "    )\n",
    "    lgbm_reg.fit(X_train, y_train)\n",
    "    \n",
    "    predict_lgbm = lgbm_reg.predict(X_test)\n",
    "    \n",
    "    rmse_lgbm, mae_lgbm, r2_lgbm = eval_metrics(y_test, predict_lgbm)\n",
    "\n",
    "    \n",
    "    ## parameters logging\n",
    "    \n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            'Nº Observations': n_observations,\n",
    "            'Nº Features': n_features,\n",
    "            'Learning Rate': learning_rate_lgbm,\n",
    "            'Max Depth': max_depth_lgbm,\n",
    "            'Booster': booster_lgbm\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ## metrics logging\n",
    "    \n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            'RMSE': rmse_lgbm,        \n",
    "            'MAE': mae_lgbm,\n",
    "            'R²': r2_lgbm\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ## model logging\n",
    "    \n",
    "    mlflow.lightgbm.log_model(lgbm_reg, 'Lightgbm')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating the context\n",
    "\n",
    "with mlflow.start_run(experiment_id=exp_id, run_name='notebook_run'):\n",
    "    \n",
    "    ## setting tags\n",
    "    \n",
    "    mlflow.set_tags(\n",
    "        {\n",
    "            'Problem': 'Regression',\n",
    "            'Models': 'Xgboost'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ## this plot isn't for statistical analysis, but for a reference when looking at the UI\n",
    "    \n",
    "    sns.displot(data=properties_clean, x='footage', col='city', col_wrap=2, kde=True,\n",
    "        kind='hist', log_scale=False, element='step', height=5, aspect=1\n",
    "    ).set_axis_labels('Footage (m²)')\n",
    "    plt.savefig('/home/garcia-ln/Documentos/real-state-prices/images/footage_hist.png')\n",
    "    \n",
    "    ## artifacts logging (images)\n",
    "    \n",
    "    mlflow.log_artifact('/home/garcia-ln/Documentos/real-state-prices/images/')\n",
    "    \n",
    "\n",
    "    ## xgboost model\n",
    "    \n",
    "    xgb_reg = XGBRegressor(\n",
    "        learning_rate=learning_rate_xgb, \n",
    "        max_depth=max_depth_xgb, \n",
    "        booster=booster_xgb, \n",
    "        eval_metric=eval_metric, \n",
    "        seed=seed\n",
    "    )\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    \n",
    "    predict_xgb = xgb_reg.predict(X_test)\n",
    "    \n",
    "    rmse_xgb, mae_xgb, r2_xgb = eval_metrics(y_test, predict_xgb)\n",
    "    \n",
    "    ## parameters logging\n",
    "    \n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            'Nº Observations': n_observations,\n",
    "            'Nº Features': n_features,\n",
    "            'Learning Rate': learning_rate_xgb,\n",
    "            'Max Depth': max_depth_xgb,\n",
    "            'Booster': booster_xgb\n",
    "        }\n",
    "    )    \n",
    "    ## metrics logging\n",
    "    \n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            'RMSE': rmse_xgb,        \n",
    "            'MAE': mae_xgb,\n",
    "            'R²': r2_xgb\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ## model logging\n",
    "    \n",
    "    mlflow.xgboost.log_model(xgb_reg, 'XGBoost')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d51eaf2bab5ade07ebe00b6ad723ed46cec5b86ecd5c27970a62f590a707991f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
